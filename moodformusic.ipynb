{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9a74c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.10.0\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-win_amd64.whl (455.9 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorflow==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorflow==2.10.0) (52.0.0.post20210125)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorflow==2.10.0) (1.15.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-win_amd64.whl (896 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorflow==2.10.0) (1.12.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorflow==2.10.0) (1.24.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorflow==2.10.0) (20.9)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.65.1-cp38-cp38-win_amd64.whl (4.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorflow==2.10.0) (3.7.4.3)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.36.2)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.0.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (8.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from packaging->tensorflow==2.10.0) (2.4.7)\n",
      "Installing collected packages: tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.65.1 keras-2.10.0 keras-preprocessing-1.1.2 libclang-18.1.1 markdown-3.6 opt-einsum-3.3.0 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e29b48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.21.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (1.21.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba12219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_moods.csv', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the path to the uploaded zip file and extraction directory\n",
    "zip_file_path = 'C:/Users/91877/Downloads/MoodforMusic.zip'  # Update this path\n",
    "extraction_dir = 'C:/path/to/extraction/directory/'  # Update this path\n",
    "\n",
    "# Create the extraction directory if it does not exist\n",
    "os.makedirs(extraction_dir, exist_ok=True)\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extraction_dir)\n",
    "\n",
    "# List the contents of the extracted directory\n",
    "extracted_files = os.listdir(extraction_dir)\n",
    "print(extracted_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f6fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28711 images belonging to 2 classes.\n",
      "Found 7176 images belonging to 2 classes.\n",
      "Found 35887 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "898/898 [==============================] - 401s 446ms/step - loss: 0.5288 - accuracy: 0.7988 - val_loss: 0.5086 - val_accuracy: 0.8000\n",
      "Epoch 2/10\n",
      "898/898 [==============================] - 387s 431ms/step - loss: 0.5068 - accuracy: 0.8000 - val_loss: 0.5015 - val_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "898/898 [==============================] - 391s 435ms/step - loss: 0.5033 - accuracy: 0.8000 - val_loss: 0.5005 - val_accuracy: 0.8000\n",
      "Epoch 4/10\n",
      "898/898 [==============================] - 426s 475ms/step - loss: 0.4980 - accuracy: 0.7999 - val_loss: 0.5018 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "170/898 [====>.........................] - ETA: 6:21 - loss: 0.5022 - accuracy: 0.7923"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'C:/path/to/extraction/directory/'  # Update this path\n",
    "test_dir = 'C:/path/to/extraction/directory/'    # Update this path\n",
    "\n",
    "# Define image dimensions and batch size\n",
    "img_width, img_height = 128, 128\n",
    "batch_size = 32\n",
    "\n",
    "# Create ImageDataGenerator instances for training and testing\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Create generators to load images in batches\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model using the generators\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {test_acc:.2f}')\n",
    "\n",
    "# Make predictions and generate a classification report\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes, target_names=list(test_generator.class_indices.keys())))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model.save('mood_detection_model.h5')\n",
    "\n",
    "# Music Recommendation Engine\n",
    "def recommend_music(mood):\n",
    "    # Placeholder for a more sophisticated recommendation engine\n",
    "    mood_to_music = {\n",
    "        'happy': ['Happy by Pharrell Williams', 'Uptown Funk by Mark Ronson ft. Bruno Mars'],\n",
    "        'sad': ['Someone Like You by Adele', 'Fix You by Coldplay'],\n",
    "        'angry': ['Breaking the Habit by Linkin Park', 'The Way I Am by Eminem'],\n",
    "        'neutral': ['Shake It Off by Taylor Swift', 'Roar by Katy Perry']\n",
    "    }\n",
    "    return random.choice(mood_to_music.get(mood, ['No recommendations available']))\n",
    "\n",
    "# User Interface Development\n",
    "def capture_mood_and_recommend_music(image_path):\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    img = image.load_img(image_path, target_size=(img_width, img_height))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "    class_labels = list(test_generator.class_indices.keys())\n",
    "    mood = class_labels[predicted_class[0]]\n",
    "\n",
    "    print(f'Detected mood: {mood}')\n",
    "    recommended_song = recommend_music(mood)\n",
    "    print(f'Recommended song: {recommended_song}')\n",
    "\n",
    "# Integration: capturing mood from an image and recommending music\n",
    "image_path = 'path/to/sample/image.jpg'  # Update this path\n",
    "capture_mood_and_recommend_music(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c044c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction and Model Training\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4538c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Model Evaluation\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.2f}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
